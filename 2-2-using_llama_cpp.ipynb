{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Llama\n",
    "\n",
    "Now that we have some knowledge of the fundamentals of how llama 2 is representing data, both within its model\n",
    "architecture itself as well as the sequences of tokens coming in and out of the model, we can start to query the model.\n",
    "Now, I've used the word \"query\" here, but I don't mean it in the relational database sense, but in the human language\n",
    "sense: to ask a question. A database query is deterministic -- if the data hasn't changed and the query hasn't changed\n",
    "then the same result is always returned. But a language model is a bit more nondeterministic, a more precise word to\n",
    "describe our interaction with the model is \"inference\", or \"predict\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /data/llama-2-13b.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  8801.63 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   400.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1324\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# Let's write our first llama.cpp based application.\n",
    "# To create a new model to query we have to identify the quantized\n",
    "# file which we want to use. In this course we have two models created\n",
    "# for experimentation, one being the base 13B paramneter model and the\n",
    "# other being the chat-tuned 13B parameter model. We'll explore the\n",
    "# base model first.\n",
    "\n",
    "# Read in the path for the model file\n",
    "import os\n",
    "\n",
    "model_path: str = os.environ[\"LLAMA_13B\"]\n",
    "\n",
    "# Import the llama.cpp python bindings and load the model\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model: Llama = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all we've done is load the model and llama.cpp has given us a ton of debug information! This is worth taking a quick\n",
    "look at, especially if you might end up using multiple kinds of models, either different architectures or different\n",
    "quantization levels.\n",
    "\n",
    "We see that this is a llama 2 model, that the context length is up to 4096 tokens - and we'll talk about that in a\n",
    "moment - some information about the special tokens which exist for the moment, and if we scroll down we can see the\n",
    "number of layers in the model, which speaks to the internal data structure, and whether these have been configured to be\n",
    "offloaded onto a GPU or not. One of the interesting options in llama.cpp is that you don't have to choose between the\n",
    "CPU or the GPU, you can do a bit of both depending upon your hardware setup.\n",
    "\n",
    "As we get to the bottom here we see the `llama_new_context_with_model` log lines, and this indicates the default seems\n",
    "to be a context of 521 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '178 miles from the Canadian border and 568 miles from New', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    }
   ],
   "source": [
    "# Before we go further I want to reduce the verbosity of the output\n",
    "# a bit -- feel free to leave this set to True to see more of the\n",
    "# nitty gritty details of what's happening!\n",
    "model.verbose = False\n",
    "\n",
    "# Now that we have a model let's actually do some inference! The\n",
    "# method we use for this is called create_completion(), and by\n",
    "# default we only need to pass it a prompt to complete and it\n",
    "# returns to us a Completion object, which is a TypedDict.\n",
    "\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "result: Completion = model.create_completion(prompt=\"The capital of Michigan is \")\n",
    "\n",
    "# The Completion type has a choices key which shows us the list of\n",
    "# responses the LLM generated, let's take a look\n",
    "print(result[\"choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So, if you are following along in the notebook you undoubtedly have a different response than I'm showing here.\n",
    "That's what we mean by inference, the model is making a series of probabilistic choices based on the input sequence and\n",
    "the weights to generate this output sequence. We can make the model behave more deterministically by setting it's\n",
    "`temperature`, which is a parameter from zero to one where values closer to zero cause the model will behave more\n",
    "deterministically and values closer to one cause the model to behave more non-deterministic, and creative. Let's do a\n",
    "little experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp=0.0, run=0, result: 8 planets, 5 dwarf planets and 123\n",
      "temp=0.0, run=1, result: 8 planets, 5 dwarf planets and 123\n",
      "temp=0.0, run=2, result: 8 planets, 5 dwarf planets and 123\n",
      "temp=0.5, run=0, result: 8 planets and 1 dwarf planet. They are Mercury,\n",
      "temp=0.5, run=1, result: 8 planets and one dwarf planet.\n",
      "The inner planets are\n",
      "temp=0.5, run=2, result: 8 planets, and Pluto is no longer considered a planet.\n",
      "The\n",
      "temp=1.0, run=0, result: 8 major planets. It is believed that there are billions of galaxies and\n",
      "temp=1.0, run=1, result: 8 planets and some dwarf planets, comets, and astero\n",
      "temp=1.0, run=2, result: 8 planets. But what are these planets called? You must have heard\n"
     ]
    }
   ],
   "source": [
    "# Let's try a few different temperature values\n",
    "temps: list[float] = [0.0, 0.5, 1.0]\n",
    "\n",
    "# Now, for each of these temperatures, let's do three completions\n",
    "prompt: str = \"The planets in the solar system include \"\n",
    "for temp in temps:\n",
    "    for i in range(0, 3):\n",
    "        result: Completion = model.create_completion(prompt=prompt, temperature=temp)\n",
    "        print(f'temp={temp}, run={i}, result: {result[\"choices\"][0][\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! We can see that the low temperature returns really consistent results, and that the high remperature gives\n",
    "different kinds of answers. Now, don't go reading into the tea leaves too much here, but play with this, experiment and\n",
    "get a feeling for how the temperature effects your prompt completions. Underneath the temperature is changing how the\n",
    "next token is picked from a set of candidate tokens, so higher levels of temperature will deviate more quickly from one\n",
    "another on repeated querying.\n",
    "\n",
    "You've undoubtedly noticed that we're just getting short little responses here. Generating tokens is slow, and by\n",
    "default the `chat_completion()` method limits the number of tokens returned to just 16. We can change this to -1 to\n",
    "generate as many tokens as available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 moons, dwarf planets and other objects.\n",
      "The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.\n",
      "Mercury is closest to the sun but it's size makes it appear smaller than our moon.\n",
      "Venus is next closest but it has a dense atmosphere which makes it appear larger than it really is.\n",
      "Earth has a thick atmosphere but it has a molten core that makes it appear larger than it really is.\n",
      "Mars has a thin atmosphere which makes it appear smaller than it really is.\n",
      "Jupiter has a thick atmosphere that makes it appear larger than it really is.\n",
      "Saturn has a thick atmosphere that makes it appear larger than it really is.\n",
      "Uranus has a thick atmosphere that makes it appear larger than it really is.\n",
      "Neptune has a thick atmosphere that makes it appear larger than it really is.\n",
      "Pluto has a thick atmosphere that makes it appear larger than it really is.\n"
     ]
    }
   ],
   "source": [
    "# Let's just do one run, and we'll leave the temperature at\n",
    "# it's default value, which is 0.8\n",
    "result: Completion = model.create_completion(prompt=prompt, max_tokens=-1)\n",
    "print(result[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the first thing you'll notice, especially if you are following along in the lab workspaces with me, is that it's\n",
    "slow. We're already running this model in a quantized form, so to speed things up further we need to either get better\n",
    "hardware, or further reduce the model size, perhaps through additional quantization. You'll also likely notice -- and I\n",
    "say likely because everything here is non-deterministic -- that the model still doesn't just \"finish\". It trails off\n",
    "eventually, but not at 16 characters. And this comes down to the content length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Length\n",
    "\n",
    "Once you understand tokenization the context length is really simple, it's just the maximum length of the token sequence\n",
    "that the model is trained on. In the case of the original llama model the context length was just over two thousand\n",
    "tokens (2,024), which means the input data used for training the model was broken up into a maximum this length of\n",
    "sequence. For llama 2 that was increased to over four thousand tokens (4,096).\n",
    "\n",
    "You can think of the context length like the amount of \"memory\" that an LLM has for a given query. The bigger it is the\n",
    "more you can put in the query and thus the more the LLM will be aware of when return to you a response. Training -- and\n",
    "inference -- with a large context length can increase quality of the output, but they do so at the cost of increased\n",
    "computation. When you create a new `Llama()` object in llama.cpp the default maximum context length is set to 512\n",
    "tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 miles of coastline, 40,000 lakes and rivers, 360 species of birds, 190 species of fish, 70 mammal species, 75 reptile species,\n",
      " more than 3,200 miles of shoreline, and 8 million acres of forest.\n",
      "The largest city in Michigan is Detroit. Other major cities are Grand Rapids, Warren, Sterling Heights, Lansing,\n",
      " Ann Arbor and Flint.\n",
      "Michigan is known for its four distinct seasons. There is something to do here year-round.\n",
      "Michigan’s largest industry is automobile manufacturing. The Big Three – General Motors, Ford\n",
      " Motor Company and Chrysler – all have headquarters here.\n",
      "Michigan is a popular vacation destination. More than 48 million people visit the state each year.\n",
      "Michigan has more than 100 lighthouses. The\n",
      " oldest is the Huron Light Station on Lake Superior. The tallest is the White River Light Station on Lake Michigan.\n",
      "Michigan is home to several world-renowned universities, including the University of Michigan and Michigan State University.\n",
      "\n",
      "The state has more than 100 breweries. The largest is Bell’s Brewery.\n",
      "Michigan is a great place to live. The cost of living is relatively low, there are plenty of jobs, and it’\n",
      "s a beautiful state.\n",
      "Michigan is a great place to visit. There are many different activities to choose from, including hiking, biking, fishing, golfing, and more.\n",
      "Michigan is a great place to\n",
      " live. The weather is mild, there are plenty of job opportunities, and it’s a great place to raise a family.\n",
      "Michigan is a great place to visit. There are many different attractions to choose from, including museum\n",
      "s, parks, beaches, and more.\n",
      "Michigan is a great place to live. The cost of living is relatively low, there are plenty of job opportunities, and it’s a great place to raise a family."
     ]
    }
   ],
   "source": [
    "# Let's do one last demo, and here I want to show you how we don't have to\n",
    "# wait for whole query to finish, but can instead use the streaming features\n",
    "# of llama.cpp to see tokens as they are completed.\n",
    "\n",
    "# I'm going to create a new model with a nice large context size\n",
    "model: Llama = Llama(model_path=model_path, verbose=False, n_ctx=4096)\n",
    "\n",
    "# If we pass the stream=True parameter to create_completion() we will get back\n",
    "# an iterator of CreatCompletionStreamResponse objects, which are just typed\n",
    "# dictionaries similar to the Completion type\n",
    "token_count: int = 0\n",
    "for result in model.create_completion(\n",
    "    prompt=\"Some fun things to do for vacation in the state of Michigan includes \",\n",
    "    max_tokens=-1,\n",
    "    stream=True,\n",
    "):\n",
    "    # I'm only going to print a newline every 50 tokens or so\n",
    "    if token_count % 50 == 0:\n",
    "        print(\"\")\n",
    "    token_count = token_count + 1\n",
    "    print(result[\"choices\"][0][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, while you learn a little bit more about all the wonderful things you can do on vacation here in the State of\n",
    "Michigan, I'm going to go grab a diet coke and refresh my voice -- I'll see you in the next video!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
